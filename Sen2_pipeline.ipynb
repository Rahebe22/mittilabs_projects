{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981374d-1585-4dfc-babd-3861b2310a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"dask[distributed]\"\n",
    "!pip install planetary-computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4396d-118a-4d3d-b674-e54a0c87d324",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "063905a5-b006-404d-8339-398f9ab8f664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import pystac_client\n",
    "import stackstac\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "from pystac_client import Client\n",
    "from stackstac import stack\n",
    "\n",
    "from rasterio.enums import Resampling\n",
    "import planetary_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ff41a-fa5f-4b6f-ac5e-11d18441854c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1ae365a-795d-47e1-bca9-87a7c1500e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_utm_epsg(lat, lon):\n",
    "    zone = int((lon + 180) / 6) + 1\n",
    "    if lat >= 0:\n",
    "        return 32600 + zone  # Northern Hemisphere\n",
    "    else:\n",
    "        return 32700 + zone  # Southern Hemisphere\n",
    "\n",
    "\n",
    "def extract_month(filename):\n",
    "    match = re.match(r'(\\d{4}_\\d{2})/raster_\\d{2}_\\d+\\.tif$', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d9267-427f-49cc-87a0-a1f860a9dfab",
   "metadata": {},
   "source": [
    "## Making grids, skip if you already have the grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e761b65b-2108-4497-9ff1-c1c27f6e80f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# india_boundary = gpd.read_file(\"./india_boundary.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b32869-4f25-4fc4-a841-3626dd0e5ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 188 grid tiles.\n"
     ]
    }
   ],
   "source": [
    "# utm_crs = india_boundary.estimate_utm_crs()\n",
    "# india_boundary = india_boundary.to_crs(utm_crs)\n",
    "\n",
    "# # 10 meters per pixel → 2240m grid size\n",
    "# grid_size = 2240\n",
    "\n",
    "# minx, miny, maxx, maxy = india_boundary.total_bounds\n",
    "\n",
    "# grid_tiles = []\n",
    "# for x in np.arange(minx, maxx, grid_size):\n",
    "#     for y in np.arange(miny, maxy, grid_size):\n",
    "#         grid_tiles.append(box(x, y, x + grid_size, y + grid_size))\n",
    "\n",
    "# grid_gdf = gpd.GeoDataFrame({\"geometry\": grid_tiles}, crs=utm_crs)\n",
    "\n",
    "# india_grid = gpd.clip(grid_gdf, india_boundary)\n",
    "\n",
    "# india_grid = india_grid.to_crs(epsg=4326)\n",
    "\n",
    "# india_grid.to_file(\"india_grid.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# print(f\"Created {len(india_grid)} grid tiles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f5ece-40fe-4d62-9d24-e760ef350a20",
   "metadata": {},
   "source": [
    "### Generating random grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbd63d-29f5-4803-8fa8-dd0be2e5dd1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load data \n",
    "# india_grid = gpd.read_file(\"india_grid.geojson\")\n",
    "# bboxes = gpd.read_file(\"bbox.geojson\")  \n",
    "\n",
    "# #  Ensure CRS match\n",
    "# if india_grid.crs != bboxes.crs:\n",
    "#     bboxes = bboxes.to_crs(india_grid.crs)\n",
    "\n",
    "# # Select all grids intersecting the bounding boxes (fully or partially) \n",
    "# bbox_union = bboxes.unary_union\n",
    "# grids_in_bbox = india_grid[india_grid.geometry.intersects(bbox_union)]\n",
    "\n",
    "# # Identify square-like grids (not just 4-sided, but width ≈ height) \n",
    "# def is_square_like(geom, tolerance=0.1):\n",
    "#     if geom.geom_type != \"Polygon\":\n",
    "#         return False\n",
    "#     coords = list(geom.exterior.coords)\n",
    "#     if len(coords) - 1 != 4:\n",
    "#         return False\n",
    "#     minx, miny, maxx, maxy = geom.bounds\n",
    "#     width = maxx - minx\n",
    "#     height = maxy - miny\n",
    "#     if height == 0:\n",
    "#         return False\n",
    "#     aspect_ratio = width / height\n",
    "#     return abs(aspect_ratio - 1) <= tolerance  # e.g. between 0.90 and 1.1\n",
    "\n",
    "# # Apply the square-like check\n",
    "# perfect_grids = india_grid[india_grid.geometry.apply(is_square_like)]\n",
    "\n",
    "# # Remove any square-like grids that intersect with the bounding boxes \n",
    "# perfect_grids_outside = perfect_grids[~perfect_grids.geometry.intersects(bbox_union)]\n",
    "\n",
    "# # Sampling logic \n",
    "# TOTAL = 300_000\n",
    "# n_from_outside = TOTAL - len(grids_in_bbox)\n",
    "\n",
    "# if n_from_outside < 0:\n",
    "#     raise ValueError(f\"Too many bbox-intersecting grids: {len(grids_in_bbox)} exceeds total {TOTAL}\")\n",
    "\n",
    "# grids_outside_random = perfect_grids_outside.sample(n=n_from_outside, random_state=42)\n",
    "\n",
    "# # Combine and save\n",
    "# final_grids = gpd.GeoDataFrame(\n",
    "#     pd.concat([grids_in_bbox, grids_outside_random], ignore_index=True),\n",
    "#     crs=india_grid.crs\n",
    "# )\n",
    "\n",
    "# final_grids.to_file(\"india_random_grids.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "\n",
    "# print(f\" Final grid count: {len(final_grids)}\")\n",
    "# print(f\" BBox-intersecting grids (all kept): {len(grids_in_bbox)}\")\n",
    "# print(f\" Random perfect-square grids from outside: {len(grids_outside_random)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a0959-0453-4165-a406-c669585d9ae4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### check the grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89e114e-ac65-4f58-9df8-f62b0d500651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2f27ba8b074c67936e960d576a3a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[22.5, 78.9], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_out…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# random_grids = gpd.read_file(\"./irrigation_raw_labels/partial_labels_grids.geojson\")\n",
    "# grid = random_grids  \n",
    "\n",
    "# ma = leafmap.Map(center=[22.5, 78.9], zoom=4)\n",
    "# ma.add_basemap('SATELLITE')\n",
    "# ma.add_geojson(random_grids)\n",
    "# ma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4a502-34a4-45ea-865e-f29a22a0645a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downloading S2 pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15572c8d-4d6d-4147-b573-99c89fbdcec9",
   "metadata": {},
   "source": [
    "### Load and Prepare India Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e79a608d-dc22-40ab-ac92-6032f792d269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "geojson_path = \"./rice_practices/mask_ready/partial_labels_grids_FID.geojson\"\n",
    "output_folder = \"./rice_practices/Sentinel2_UTM_TimeSeries_new\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# STAC catalog earth_search API\n",
    "catalog = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "\n",
    "# Assets to download\n",
    "assets = ['green', 'nir', 'red', 'rededge1', 'swir1', 'scl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf74e2f-a6ef-4b23-9413-4d8777c489f2",
   "metadata": {},
   "source": [
    "### Search, Stack, Composite, and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2e413-353a-429a-b388-b7afb2cae23c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: tile000000 | 2024_07\n",
      "2024-07-01/2024-07-30\n"
     ]
    }
   ],
   "source": [
    "# Load grid with FID, filename, and geometry\n",
    "grids = gpd.read_file(geojson_path)\n",
    "grids[\"FID\"] = grids[\"FID\"].astype(str)\n",
    "\n",
    "# Identify unique tile-date combinations\n",
    "needed_downloads = set()\n",
    "for _, row in grids.iterrows():\n",
    "    fid = row[\"FID\"]                       # e.g., tile000000\n",
    "    yyyymm = row[\"filename\"].split(\"/\")[0]  # e.g., 2024_07\n",
    "    needed_downloads.add((fid, yyyymm))\n",
    "\n",
    "# Process each tile-month\n",
    "for fid, yyyymm in sorted(needed_downloads):\n",
    "    print(f\"\\nProcessing: {fid} | {yyyymm}\")\n",
    "    year, month = yyyymm.split(\"_\")\n",
    "    date_range = f\"{year}-{month}-01/{year}-{month}-30\"\n",
    "    print(date_range)\n",
    "\n",
    "    # Get geometry from matching row\n",
    "    match = grids[(grids[\"FID\"] == fid) & (grids[\"filename\"].str.contains(yyyymm))]\n",
    "    if match.empty:\n",
    "        print(f\"No matching geometry for {fid} in {yyyymm}, skipping.\")\n",
    "        break\n",
    "\n",
    "    row = match.iloc[0]\n",
    "    grid_geom = row.geometry\n",
    "    centroid = grid_geom.centroid\n",
    "    utm_epsg = get_utm_epsg(centroid.y, centroid.x)\n",
    "\n",
    "    # Search STAC\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        intersects=grid_geom,\n",
    "        datetime=date_range,\n",
    "        limit=10\n",
    "    )\n",
    "\n",
    "    items = search.get_all_items()\n",
    "    if not items:\n",
    "        print(f\"No images for {fid} in {yyyymm}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    item = items[0]\n",
    "    if \"proj:code\" not in item.properties:\n",
    "        print(f\"No EPSG info in item metadata for {fid}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    sentinel_epsg = int(item.properties[\"proj:code\"].replace(\"EPSG:\", \"\"))\n",
    "\n",
    "    # Stack\n",
    "    stack = stackstac.stack(\n",
    "        [item],\n",
    "        assets=[\"B02\", \"B03\", \"B04\", \"B08\", \"scl\"],\n",
    "        epsg=sentinel_epsg,\n",
    "        resolution=10,\n",
    "        chunksize=2048\n",
    "    )\n",
    "\n",
    "    grid_utm = gpd.GeoSeries(grid_geom, crs=grids.crs).to_crs(f\"EPSG:{sentinel_epsg}\").geometry[0]\n",
    "    clipped = stack.rio.clip([grid_utm], f\"EPSG:{sentinel_epsg}\", drop=True)\n",
    "\n",
    "    # Quality filter\n",
    "    scl = clipped.sel(band=\"scl\")\n",
    "    valid_mask = scl.isin([4, 5, 6, 7])\n",
    "    valid_ratio = valid_mask.sum() / (valid_mask.sizes[\"x\"] * valid_mask.sizes[\"y\"])\n",
    "\n",
    "    if valid_ratio < 0.6:\n",
    "        print(f\"Low valid pixel ratio ({valid_ratio:.2f}) for {fid}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Apply mask and get median\n",
    "    clipped = clipped.where(valid_mask, np.nan)\n",
    "    median_img = clipped.median(dim=\"time\", keep_attrs=True)\n",
    "\n",
    "    # Remove SCL band\n",
    "    if \"scl\" in median_img.band.values:\n",
    "        median_img = median_img.sel(band=median_img.band != \"scl\")\n",
    "\n",
    "    # Resample to 224×224\n",
    "    median_img = median_img.rio.reproject(\n",
    "        dst_crs=f\"EPSG:{sentinel_epsg}\",\n",
    "        shape=(224, 224),\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "\n",
    "    # Save with correct naming\n",
    "    output_name = f\"{fid}_{yyyymm}_s2.tif\"\n",
    "    output_path = os.path.join(output_folder, output_name)\n",
    "\n",
    "    median_img.rio.to_raster(output_path)\n",
    "    print(f\" Saved: {output_path} | Shape: {median_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb1b19-1ae1-41f1-801a-383cb8f06f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
